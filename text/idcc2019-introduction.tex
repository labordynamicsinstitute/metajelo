Reproducibility and replicability of scientific findings has been given great scrutiny in recent years \parencite{CamererScience2016,Collaboration2015-ev,Klein2014,FanelliPNAS2018}.%
\footnote{There is considerable heterogeneity in the use of the terms
	``reproducibility'' and ``replicability''. In this paper, we will adopt
	the following definitions: reproducibility is ``the ability of a
	researcher to duplicate the results of a prior study using the same
	materials and procedures as were used by the original investigator,''
	\parencite{Bollen2015} whereas replicability
	differs in that ``new data are collected.'' (ibidem). See also \cite{NationalAcademiesofSciencesEngineeringandMedicine2019} for a similar definition.}
%
Actual published individual reproductions or replications are notably not very common \parencite[in economics, see][]{BellMiller2013b,Duvendack2017}. In part, this is because it often was difficult to find the materials required to conduct reproducibility or replication exercises \parencite{Dewald1986,McCullough2006,McCullough03}.  

Scientific journals, whether run by publishing companies (Springer, Elsevier, etc.) or learned societies (American Economic Association, Midwest Political Science Association, American Statistical Association, Royal Statistical Society, to name just a few in the social and statistical sciences), have been playing an important role in supporting these efforts for many years \parencite{stodden_enhancing_2016}, and continue to explore novel and better ways of doing so. More and more journals are adopting ``data and code availability'' policies,%
\footnote{In the social sciences, the major economics and political science journals published \acp{DCAP} in the mid-2000s \parencite{American_Economic_Association2008-wayback,nicholaseubankThePoliticalMethodologist2014}.}
though some doubt has been cast on their effectiveness \parencite{stodden_toward_2013,Stoddenempiricalanalysisjournal2018,Hoeffler2017,ChangAm.Econ.Rev.2017}. Some of the lack of replicability identified by recent studies \parencite{Hoeffler2017a,Chang2017,ChangLi2015,CamererEvaluatingreplicabilitylaboratory2016,Stoddenempiricalanalysisjournal2018}  occurs despite the fact that journals have  policies that encourage it. One issue is the lack of consistent, reliable metadata on the materials provided to journals, and in particular those supplementary materials, such as data and code, provided through third-party locations.

Several journals have been hosting these ``supplementary materials'' on their own journal websites or on affiliated repositories (e.g., Harvard Dataverse, Figshare) in support of reproducibility of the work described in published scientific articles. In these cases, data and code deposits are requested when authors' work has been (conditionally) accepted after peer review, or, less frequently, as part of the original manuscript submission process. By doing so, these journals assume for themselves (or delegate to a single trusted third party) the curation role for these materials, and assume control of how long these materials are to be preserved and their terms of accessibility.

Some of the lack of replicability identified by recent studies \parencite{Hoeffler2017a,Chang2017,ChangLi2015,CamererScience2016,StoddenPNAS2018}  occurs despite the fact that journals have  policies that encourage the provision of replication packages. Evaluating compliance with policies as well as quality and utility of replication packages is arduous, if not impossible, due to a lack of consistent, reliable metadata on the materials provided to journals. In many cases, while a replication package is provided to the journal, the underlying data are not available within the replication package, due to a mix of non-compliance, legal, and ethical constraints on redistribution of the data. 

XXXXXX leading to renewed calls for better reproducibility \parencite{stodden_enhancing_2016}, broad efforts to better define \acp{DCAP} \parencite{CenterforOpenScience2016,HrynaszkiewiczInt.J.Digit.Curation2017}, and increased enforcement of \acp{DCAP} \parencite{JacobyInsideHigherEd2017,10.1257/pandp.108.745,VilhuberAEAPap.Proc.2019}.

Authors are increasingly being encouraged and trained in reproducible methods from the outset of their research projects \citep{WilsonArXiv160900037Cs2016,Christensen2019a}, rather than describing their data and code much later, \textit{i.e.} after submission to journals. This includes carefully documenting provenance of third-party datasets being used, and properly curating generated datasets (surveys, collected data, etc.) in data archives as soon as possible. Such early deposit allows more time for curation, potentially improving the quality of deposits. However, it conflicts with some (but not all) journal workflows, which integrate data deposit into the article submission process. Prior deposits may not be captured by the same metadata as in-workflow deposits.

Furthermore, in at least some social sciences, the use of pre-existing but non-public data has increased substantially \parencite{Chetty2012} and remains high: \cite{Kingi2018} show about 40\% of economics articles using restricted-access data.  Confidentiality and licensing constraints prevent authors from depositing such data in open archives. Data citation \parencite{DataCitationSynthesisGroup2014} of such data is often challenging. Journals must rely on an increasingly diverse cadre of data-holding institutions, not all of which are or perceive themselves as archives in the traditional sense, while satisfying increasing scrutiny of the provenance of the research results published by them. 

Both scenarios - early and third-party deposit of data and use of restricted-access data - make it difficult for authors and journals to document the full provenance of the data underlying the scientific results in published articles. The resulting lack of transparency in data provenance is detrimental to the overall effort of increasing transparency in the sciences, in particular FAIRness of data access \citep{HagstromFORCE112014}.\footnote{We note that restricted access to data is not inherently incompatible with FAIR, as long as there exists metadata that is FAIR.}

The approach outlined in this article proposes a metadata package, derived from existing metadata schemata where possible, that provides a lightweight approach to ameliorating this problem. In particular, the proposed metadata package, called \metajelo (\underline{meta}data package for \underline{j}ournals to support \underline{e}xternal \underline{l}inked \underline{o}bjects) documents some of the key characteristics that journals care about in the case of supplementary materials that are held by third parties, within the context of FAIR: existence, accessibility, and permanence. Our intent in defining the metadata package is three-fold. First, the package enables  authors to provide the information as they submit articles to journals, allowing informed editorial decisions to be made. Second, at the time of publication, the information is made public, providing robust documentation on data provenance in an immutable package, in a compact fashion.  The package allows for better documentation of any data, regardless of the difficulty of access.   Thus the information provided for less accessible (non-public data) is improved by treating it symmetrically with open access data. Finally, by providing the information in a machine-readable format, the evaluation of compliance with \acp{DCAP} can be more easily assessed systematically. Overall, \metajelo aims to   increase the transparency of what up until now has been very opaque.

We start by providing some background. We describe the use case motivating our approach, with detailed use cases provided in the appendix. We relate our approach to existing metadata, both in terms of structure and of content, and then describe the metadata package. We conclude by discussing some usability issues for three contributors or consumers of this information, and an outlook on a possible implementation.